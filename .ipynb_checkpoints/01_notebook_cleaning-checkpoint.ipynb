{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh = pd.read_csv('data/ed_socio_health.csv')\n",
    "df_wp = pd.read_csv('data/wage_poverty.csv')\n",
    "df_un = pd.read_csv('data/unemployment_clean.csv')\n",
    "df_fins = pd.read_csv('data/food_ins_18.csv')\n",
    "df_ed = pd.read_csv('data/education_stats_dsi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sh.fips.astype(int)\n",
    "df_wp.fips.astype(int)\n",
    "df_un.fips.astype(int)\n",
    "df_fins.fips.astype(int)\n",
    "df_ed.fips.astype(int).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Dataframes\n",
    "Merge each dataset into one main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.merge(left = df_sh, right = df_wp, on = 'fips')\n",
    "df_m = pd.merge(left = df_m, right = df_un, on = 'fips')\n",
    "df_m = pd.merge(left = df_m, right = df_fins, on = 'fips')\n",
    "df_m = pd.merge(left = df_m, right = df_ed, on = 'fips' )\n",
    "df_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crete csv\n",
    "df_m.to_csv('data/df_full.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(df_m.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = nulls.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nulls.to_csv('./data/nulls.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming state_x as full_st_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df_m.rename(columns = {\"state_x\": \"state_name\",\n",
    "                               \"state_y\": \"state_abr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns that are unlikely to have explanatory power over and above other variables and that are duplicate information from 2016 data (eg. 2019 unemployment data).\n",
    "\n",
    "Dropping num and percent food insecure in 2016 dataset as the 2018 data has both children and total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['teen_birth_rate', 'age_adjusted_death_rate', 'child_mortality_rate', \n",
    "             'infant_mortality_rate', 'num_limited_access_to_healthy_foods', \n",
    "             'segregation_index', 'segregation_index_2', 'homicide_rate', \n",
    "             'suicide_rate_age_adjusted', 'juvenile_arrest_rate', 'area_name', \n",
    "             'num_below_poverty', 'percent_some_college', 'labor_force', \n",
    "             'percent_unemployed_CHR', 'med_inc_19', 'unemployment_rate_2019', \n",
    "             'med_household_inc_19', 'med_hh_income_percent_of_state_total_2019', \n",
    "             'num_food_insecure', 'percent_food_insecure', 'less_than_high_school_diploma', \n",
    "             'bachelor_degree_or_higher', 'percent_less_than_18_years_of_age', 'percent_65_and_over', \n",
    "            'mental_health_provider_rate']\n",
    "\n",
    "df_m = df_m.drop(columns = drop_list)\n",
    "df_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop % and convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m['fi_rate_18'] = df_m['fi_rate_18'].str.replace('%', '').astype(float)\n",
    "df_m['ch_fi_rate_18'] = df_m['ch_fi_rate_18'].str.replace('%', '').astype(float)\n",
    "df_m['cpm_18'] = df_m['cpm_18'].str.strip('US$').astype(float)\n",
    "df_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check types\n",
    "types = pd.DataFrame(df_m.dtypes)\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of missing values\n",
    "df_m.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = [col for col in df_m if df_m[col].isna().any()]\n",
    "\n",
    "df_m_mean = df_m.copy()\n",
    "df_m_median = df_m.copy()\n",
    "df_m_mode = df_m.copy()\n",
    "df_m_knn = pd.get_dummies(df_m.copy())\n",
    "df_m_iterative = pd.get_dummies(df_m.copy())\n",
    "df_m_lr = pd.get_dummies(df_m.copy())\n",
    "df_m_rf = pd.get_dummies(df_m.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer with mean, median, mode\n",
    "for col in null_columns:\n",
    "    df_m_mean[col] = df_m[col].fillna(df_m[col].dropna().mean())\n",
    "    df_m_median[col] = df_m[col].fillna(df_m[col].dropna().median())\n",
    "    df_m_mode[col] = df_m[col].fillna(df_m[col].dropna().mode()[0])\n",
    "    \n",
    "print('Mean imputation nulls: ', df_m_mean.isnull().sum().sum())\n",
    "print('Median imputation nulls: ', df_m_median.isnull().sum().sum())\n",
    "print('Mode imputation nulls: ', df_m_mode.isnull().sum().sum())\n",
    "\n",
    "imp_knn = KNNImputer(n_neighbors = 2)\n",
    "df_m_knn = imp_knn.fit_transform(df_m_knn)\n",
    "df_m_knn = pd.DataFrame(df_m_knn)\n",
    "print('Knn imputation nulls: ', df_m_knn.isnull().sum().sum())\n",
    "\n",
    "# imp_iterative = IterativeImputer(random_state = 0)\n",
    "# df_m_iterative = imp_iterative.fit_transform(df_m_iterative)\n",
    "# df_m_iterative = pd.DataFrame(df_m_iterative)\n",
    "# print('Iterative imputation nulls: ', df_m_iterative.isnull().sum().sum())\n",
    "\n",
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "def impute_missing_data(df, model):\n",
    "    for col in null_columns:\n",
    "        df_cc = df.dropna() #use complete case\n",
    "        \n",
    "        # Fit model\n",
    "        X = df_cc.drop(columns = col)\n",
    "        y = df_cc[col]\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        df_temp = df.copy()\n",
    "        \n",
    "        for column in df_temp.columns:\n",
    "            if column != col:\n",
    "                df_temp[column] = df_temp[column].fillna(df_temp[column].dropna().median())\n",
    "                \n",
    "        X_temp = df_temp.drop(columns = col) #drop target for prediction so there is no nulls\n",
    "\n",
    "        # Loop through all of the rows checking for nulls in the col column, create a pred, and set that cell equal to pred\n",
    "        for index, row in df_temp.iterrows():\n",
    "            if pd.isnull(df_temp[col].iloc[index]):\n",
    "                X_test_row = X_temp.iloc[index] #use df without target`ii\n",
    "                X_test_row = X_test_row.values.reshape(1, -1)\n",
    "                \n",
    "                pred = model.predict(X_test_row)\n",
    "                print(pred[0])\n",
    "                df[col][index] = pred[0]\n",
    "                \n",
    "df_m_lr = impute_missing_data(df_m_lr, lr)\n",
    "print('Lr imputation nulls: ', df_m_lr.isnull().sum().sum())\n",
    "\n",
    "df_m_rf = impute_missing_data(df_m_rf, rf)\n",
    "print('Rf imputation nulls: ',df_m_rf.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Features needing median to be added in place of null values:\n",
    "features=['percent_low_birthweight','primary_care_physicians_rate','mental_health_provider_rate','high_school_graduation_rate',\n",
    "'food_environment_index','percent_with_access_to_exercise_opportunities','num_households_CHR','percent_single_parent_households_CHR',\n",
    "'violent_crime_rate','percent_limited_access_to_healthy_foods','percent_enrolled_in_free_or_reduced_lunch','percent_severe_housing_cost_burden',\n",
    "'percent_rural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['percent_low_birthweight'].fillna(df_full['percent_low_birthweight'].median(), inplace=True)\n",
    "df_full['primary_care_physicians_rate'].fillna(df_full['primary_care_physicians_rate'].median(), inplace=True)\n",
    "df_full['high_school_graduation_rate'].fillna(df_full['high_school_graduation_rate'].median(), inplace=True)\n",
    "df_full['percent_with_access_to_exercise_opportunities'].fillna(df_full['percent_with_access_to_exercise_opportunities'].median(), inplace=True)\n",
    "df_full['num_households_CHR'].fillna(df_full['num_households_CHR'].median(), inplace=True)\n",
    "df_full['percent_single_parent_households_CHR'].fillna(df_full['percent_single_parent_households_CHR'].median(), inplace=True)\n",
    "df_full['violent_crime_rate'].fillna(df_full['violent_crime_rate'].median(), inplace=True)\n",
    "df_full['percent_limited_access_to_healthy_foods'].fillna(df_full['percent_limited_access_to_healthy_foods'].median(), inplace=True)\n",
    "df_full['percent_enrolled_in_free_or_reduced_lunch'].fillna(df_full['percent_enrolled_in_free_or_reduced_lunch'].median(), inplace=True)\n",
    "df_full['percent_severe_housing_cost_burden'].fillna(df_full['percent_severe_housing_cost_burden'].median(), inplace=True)\n",
    "df_full['percent_rural'].fillna(df_full['percent_rural'].median(), inplace=True)\n",
    "df_full['food_environment_index'].fillna(df_full['food_environment_index'].median(), inplace=True)\n",
    "df_full['life_expectancy'].fillna(df_full['life_expectancy'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv('./data/df_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
